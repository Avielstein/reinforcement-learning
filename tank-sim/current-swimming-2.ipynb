{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23b0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygame\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "import time\n",
    "import threading\n",
    "import collections\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# For visualization\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "import math\n",
    "\n",
    "# For RL algorithm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5dbf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1111\u001b[0m\n\u001b[1;32m   1107\u001b[0m     run_visualization(env, agent)\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1111\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 1101\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1098\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstart_training(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;66;03m# Run visualization\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_visualization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# After visualization is closed, we can save the trained model\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(agent\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfish_policy.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 1018\u001b[0m, in \u001b[0;36mRealTimeTrainer.run_visualization\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;66;03m# Update display\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 60 FPS\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m pygame\u001b[38;5;241m.\u001b[39mquit()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Constants\n",
    "TANK_SIZE = 100  # Size of the tank (square)\n",
    "CENTER_POINT = np.array([TANK_SIZE/2, TANK_SIZE/2])\n",
    "MAX_VELOCITY = 30\n",
    "MAX_FORCE = 10\n",
    "CURRENT_COUNT = 0  # Number of water currents\n",
    "MAX_CURRENT_STRENGTH = 5.0\n",
    "CURRENT_CHANGE_PROB = 0.01  # Probability of current changing direction/strength\n",
    "CURRENT_RADIUS = 30.0  # Radius of current influence\n",
    "WATER_RESISTANCE = 0.1  # Resistance factor\n",
    "TIMESTEP = 0.1  # Physics timestep\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 0.0005\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.2\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.01\n",
    "MAX_GRAD_NORM = 0.5\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 2048\n",
    "EPOCHS = 10\n",
    "UPDATE_FREQUENCY = 2048  # Steps between policy updates\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class WaterCurrent:\n",
    "    \"\"\"Represents a water current in the environment\"\"\"\n",
    "    \n",
    "    def __init__(self, tank_size: float):\n",
    "        self.tank_size = tank_size\n",
    "        self.position = np.random.uniform(0, tank_size, 2)\n",
    "        self.direction = self._random_direction()\n",
    "        self.strength = np.random.uniform(1.0, MAX_CURRENT_STRENGTH)\n",
    "        self.color = np.random.uniform(0, 1, 3)  # Random color for visualization\n",
    "        self.velocity = np.random.uniform(1.0, 3.0)  # Current movement speed\n",
    "        \n",
    "    def _random_direction(self) -> np.ndarray:\n",
    "        \"\"\"Generate a random unit vector for direction\"\"\"\n",
    "        angle = np.random.uniform(0, 2 * np.pi)\n",
    "        return np.array([np.cos(angle), np.sin(angle)])\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update current position and possibly direction/strength\"\"\"\n",
    "        # Move the current\n",
    "        self.position += self.direction * self.velocity * TIMESTEP\n",
    "        \n",
    "        # Bounce off walls\n",
    "        for i in range(2):\n",
    "            if self.position[i] < 0:\n",
    "                self.position[i] = 0\n",
    "                self.direction[i] *= -1\n",
    "            elif self.position[i] > self.tank_size:\n",
    "                self.position[i] = self.tank_size\n",
    "                self.direction[i] *= -1\n",
    "        \n",
    "        # Occasionally change direction and strength\n",
    "        if np.random.random() < CURRENT_CHANGE_PROB:\n",
    "            self.direction = self._random_direction()\n",
    "            self.strength = np.random.uniform(1.0, MAX_CURRENT_STRENGTH)\n",
    "    \n",
    "    def get_force_at(self, position: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate force exerted by this current at the given position\"\"\"\n",
    "        distance = np.linalg.norm(position - self.position)\n",
    "        \n",
    "        # If outside the influence radius, no effect\n",
    "        if distance > CURRENT_RADIUS:\n",
    "            return np.zeros(2)\n",
    "        \n",
    "        # Force decreases with distance (inverse linear relationship)\n",
    "        force_magnitude = self.strength * (1 - distance / CURRENT_RADIUS)\n",
    "        return force_magnitude * self.direction\n",
    "\n",
    "\n",
    "class FishEnv(gym.Env):\n",
    "    \"\"\"Fish environment with water currents\"\"\"\n",
    "    \n",
    "    metadata = {'render_modes': ['rgb_array']}\n",
    "    \n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # State space: fish position (x,y), fish velocity (vx,vy), and currents\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, -MAX_VELOCITY, -MAX_VELOCITY] + [0, 0, -1, -1, 0] * CURRENT_COUNT),\n",
    "            high=np.array([TANK_SIZE, TANK_SIZE, MAX_VELOCITY, MAX_VELOCITY] + [TANK_SIZE, TANK_SIZE, 1, 1, MAX_CURRENT_STRENGTH] * CURRENT_COUNT),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Action space: thrust force in x and y directions\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-MAX_FORCE,\n",
    "            high=MAX_FORCE,\n",
    "            shape=(2,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Initialize state\n",
    "        self.fish_position = None\n",
    "        self.fish_velocity = None\n",
    "        self.currents = []\n",
    "        self.steps = 0\n",
    "        self.max_steps = 1000\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        # For visualization\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        \n",
    "        # For metrics\n",
    "        self.current_episode_reward = 0\n",
    "        self.reward_history = []\n",
    "        self.distance_history = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        \n",
    "        # For trajectories\n",
    "        self.trajectory = []\n",
    "        \n",
    "        # Reset the environment\n",
    "        self.reset()\n",
    "    \n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        \"\"\"Convert current state to observation vector\"\"\"\n",
    "        obs = np.concatenate([\n",
    "            self.fish_position,\n",
    "            self.fish_velocity\n",
    "        ])\n",
    "        \n",
    "        # Add information about each current\n",
    "        for current in self.currents:\n",
    "            current_info = np.concatenate([\n",
    "                current.position,\n",
    "                current.direction,\n",
    "                [current.strength]\n",
    "            ])\n",
    "            obs = np.concatenate([obs, current_info])\n",
    "            \n",
    "        return obs\n",
    "    \n",
    "    def _get_info(self) -> Dict:\n",
    "        \"\"\"Get additional information about the state\"\"\"\n",
    "        distance_to_center = np.linalg.norm(self.fish_position - CENTER_POINT)\n",
    "        \n",
    "        return {\n",
    "            \"distance_to_center\": distance_to_center,\n",
    "            \"step\": self.steps,\n",
    "            \"total_reward\": self.current_episode_reward\n",
    "        }\n",
    "    \n",
    "    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Reset the environment to initial state\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Place fish randomly, but not in the center\n",
    "        while True:\n",
    "            self.fish_position = np.random.uniform(0, TANK_SIZE, 2)\n",
    "            if np.linalg.norm(self.fish_position - CENTER_POINT) > 20:\n",
    "                break\n",
    "                \n",
    "        self.fish_velocity = np.zeros(2)\n",
    "        \n",
    "        # Create water currents\n",
    "        self.currents = [WaterCurrent(TANK_SIZE) for _ in range(CURRENT_COUNT)]\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.current_episode_reward = 0\n",
    "        self.trajectory = [self.fish_position.copy()]\n",
    "        \n",
    "        return self._get_obs(), self._get_info()\n",
    "    \n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"Take a step in the environment given an action\"\"\"\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Apply the action (thrust force)\n",
    "        action = np.clip(action, -MAX_FORCE, MAX_FORCE)\n",
    "        \n",
    "        # Calculate water current forces\n",
    "        current_force = np.zeros(2)\n",
    "        for current in self.currents:\n",
    "            current.update()\n",
    "            current_force += current.get_force_at(self.fish_position)\n",
    "        \n",
    "        # Calculate total force and update velocity\n",
    "        total_force = action + current_force\n",
    "        water_resistance_force = -WATER_RESISTANCE * self.fish_velocity * np.abs(self.fish_velocity)\n",
    "        \n",
    "        # Update velocity using forces\n",
    "        self.fish_velocity += (total_force + water_resistance_force) * TIMESTEP\n",
    "        self.fish_velocity = np.clip(self.fish_velocity, -MAX_VELOCITY, MAX_VELOCITY)\n",
    "        \n",
    "        # Update position\n",
    "        new_position = self.fish_position + self.fish_velocity * TIMESTEP\n",
    "        \n",
    "        # Handle bouncing off walls\n",
    "        for i in range(2):\n",
    "            if new_position[i] < 0:\n",
    "                new_position[i] = 0\n",
    "                self.fish_velocity[i] *= -0.5  # Lose energy when hitting wall\n",
    "            elif new_position[i] > TANK_SIZE:\n",
    "                new_position[i] = TANK_SIZE\n",
    "                self.fish_velocity[i] *= -0.5  # Lose energy when hitting wall\n",
    "        \n",
    "        self.fish_position = new_position\n",
    "        self.trajectory.append(self.fish_position.copy())\n",
    "        \n",
    "        # Calculate reward\n",
    "        distance_to_center = np.linalg.norm(self.fish_position - CENTER_POINT)\n",
    "        \n",
    "        # Exponential reward for being close to center (higher reward when very close)\n",
    "        position_reward = 10 * np.exp(-0.05 * distance_to_center)\n",
    "        \n",
    "        # Penalty for high velocity (encourage stability)\n",
    "        velocity_penalty = -0.01 * np.linalg.norm(self.fish_velocity)\n",
    "        \n",
    "        # Penalty for using thrust (encourage efficiency)\n",
    "        thrust_penalty = -0.005 * np.linalg.norm(action)\n",
    "        \n",
    "        reward = position_reward + velocity_penalty + thrust_penalty\n",
    "        self.current_episode_reward += reward\n",
    "        \n",
    "        # For metrics\n",
    "        self.reward_history.append(reward)\n",
    "        self.distance_history.append(distance_to_center)\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.steps >= self.max_steps\n",
    "        \n",
    "        if done:\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            self.episode_lengths.append(self.steps)\n",
    "        \n",
    "        return self._get_obs(), reward, done, False, self._get_info()\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Render the environment\"\"\"\n",
    "        if self.render_mode == 'rgb_array':\n",
    "            return self._render_frame()\n",
    "    \n",
    "    def _render_frame(self):\n",
    "        \"\"\"Render a frame for visualization\"\"\"\n",
    "        if self.screen is None:\n",
    "            # Initialize pygame screen\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.screen = pygame.Surface((TANK_SIZE * 7, TANK_SIZE * 7))\n",
    "            self.clock = pygame.time.Clock()\n",
    "        \n",
    "        self.screen.fill((255, 255, 255))\n",
    "        \n",
    "        # Scale factor for visualization\n",
    "        scale = 7\n",
    "        \n",
    "        # Draw border\n",
    "        pygame.draw.rect(self.screen, (0, 0, 0), pygame.Rect(0, 0, TANK_SIZE * scale, TANK_SIZE * scale), 2)\n",
    "        \n",
    "        # Draw target center\n",
    "        pygame.draw.circle(self.screen, (255, 200, 200), (CENTER_POINT[0] * scale, CENTER_POINT[1] * scale), 10)\n",
    "        \n",
    "        # Draw currents (as circles with arrows)\n",
    "        for current in self.currents:\n",
    "            x, y = current.position * scale\n",
    "            \n",
    "            # Draw current influence area\n",
    "            transparency = 50  # Alpha value\n",
    "            s = pygame.Surface((CURRENT_RADIUS * 2 * scale, CURRENT_RADIUS * 2 * scale), pygame.SRCALPHA)\n",
    "            color = tuple(int(c * 255) for c in current.color) + (transparency,)\n",
    "            pygame.draw.circle(s, color, (CURRENT_RADIUS * scale, CURRENT_RADIUS * scale), CURRENT_RADIUS * scale)\n",
    "            self.screen.blit(s, (x - CURRENT_RADIUS * scale, y - CURRENT_RADIUS * scale))\n",
    "            \n",
    "            # Draw arrow showing direction and strength\n",
    "            end_x = x + current.direction[0] * current.strength * 5\n",
    "            end_y = y + current.direction[1] * current.strength * 5\n",
    "            pygame.draw.line(self.screen, (0, 0, 0), (x, y), (end_x, end_y), 2)\n",
    "            \n",
    "            # Draw arrowhead\n",
    "            arrow_length = 7\n",
    "            angle = math.atan2(end_y - y, end_x - x)\n",
    "            pygame.draw.polygon(self.screen, (0, 0, 0), [\n",
    "                (end_x, end_y),\n",
    "                (end_x - arrow_length * math.cos(angle - math.pi/6), end_y - arrow_length * math.sin(angle - math.pi/6)),\n",
    "                (end_x - arrow_length * math.cos(angle + math.pi/6), end_y - arrow_length * math.sin(angle + math.pi/6))\n",
    "            ])\n",
    "        \n",
    "        # Draw fish\n",
    "        fish_x, fish_y = self.fish_position * scale\n",
    "        fish_size = 6\n",
    "        \n",
    "        # Draw fish body as a circle\n",
    "        pygame.draw.circle(self.screen, (0, 100, 255), (fish_x, fish_y), fish_size)\n",
    "        \n",
    "        # Draw fish \"face\" showing direction of motion\n",
    "        if np.linalg.norm(self.fish_velocity) > 0.1:\n",
    "            direction = self.fish_velocity / np.linalg.norm(self.fish_velocity)\n",
    "            eye_x = fish_x + direction[0] * fish_size * 0.7\n",
    "            eye_y = fish_y + direction[1] * fish_size * 0.7\n",
    "            pygame.draw.circle(self.screen, (255, 255, 255), (eye_x, eye_y), fish_size * 0.4)\n",
    "        \n",
    "        # Convert to RGB array\n",
    "        observation = np.transpose(\n",
    "            np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "        )\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"Replay buffer for storing experience\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add experience to buffer\"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample random batch of experiences\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Actor-Critic network for PPO algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_size: int, action_size: int):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor (policy) head\n",
    "        self.actor_mean = nn.Linear(128, action_size)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(action_size))\n",
    "        \n",
    "        # Critic (value) head\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through network\"\"\"\n",
    "        x = torch.FloatTensor(x)\n",
    "        shared_features = self.shared(x)\n",
    "        \n",
    "        # Actor output (action mean and std)\n",
    "        action_mean = self.actor_mean(shared_features)\n",
    "        action_std = torch.exp(self.actor_logstd)\n",
    "        \n",
    "        # Critic output (state value)\n",
    "        value = self.critic(shared_features)\n",
    "        \n",
    "        return action_mean, action_std, value\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        \"\"\"Sample action from policy\"\"\"\n",
    "        with torch.no_grad():\n",
    "            action_mean, action_std, _ = self.forward(state)\n",
    "            \n",
    "            if deterministic:\n",
    "                return action_mean.numpy()\n",
    "            \n",
    "            dist = Normal(action_mean, action_std)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            \n",
    "            return action.numpy(), log_prob.numpy()\n",
    "    \n",
    "    def evaluate_actions(self, states, actions):\n",
    "        \"\"\"Evaluate actions for PPO update\"\"\"\n",
    "        action_mean, action_std, values = self.forward(states)\n",
    "        \n",
    "        dist = Normal(action_mean, action_std)\n",
    "        log_probs = dist.log_prob(actions).sum(dim=-1)\n",
    "        entropy = dist.entropy().sum(dim=-1).mean()\n",
    "        \n",
    "        return log_probs, values.squeeze(), entropy\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"PPO Agent implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        obs_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.shape[0]\n",
    "        \n",
    "        self.policy = PolicyNetwork(obs_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        self.memory = []\n",
    "        self.training_step = 0\n",
    "        \n",
    "        self.running_rewards = collections.deque(maxlen=100)\n",
    "        self.running_lengths = collections.deque(maxlen=100)\n",
    "        \n",
    "        # Metrics\n",
    "        self.avg_rewards = []\n",
    "        self.avg_lengths = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.entropies = []\n",
    "    \n",
    "    def store_transition(self, state, action, action_log_prob, reward, done, value):\n",
    "        \"\"\"Store transition in memory\"\"\"\n",
    "        self.memory.append((state, action, action_log_prob, reward, done, value))\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy using PPO algorithm\"\"\"\n",
    "        memory = self.memory\n",
    "        self.memory = []\n",
    "        \n",
    "        states = np.array([m[0] for m in memory])\n",
    "        actions = np.array([m[1] for m in memory])\n",
    "        old_log_probs = np.array([m[2] for m in memory])\n",
    "        rewards = np.array([m[3] for m in memory])\n",
    "        dones = np.array([m[4] for m in memory])\n",
    "        values = np.array([m[5] for m in memory])\n",
    "        \n",
    "        # Compute advantages using GAE\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            if dones[i]:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = values[i+1] if i < len(rewards)-1 else 0\n",
    "                \n",
    "            delta = rewards[i] + GAMMA * next_value * (1 - dones[i]) - values[i]\n",
    "            gae = delta + GAMMA * GAE_LAMBDA * (1 - dones[i]) * gae\n",
    "            \n",
    "            returns.insert(0, gae + values[i])\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        returns = np.array(returns)\n",
    "        advantages = np.array(advantages)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        advantages = torch.FloatTensor(advantages)\n",
    "        \n",
    "        # PPO update loop\n",
    "        policy_loss_epoch = 0\n",
    "        value_loss_epoch = 0\n",
    "        entropy_epoch = 0\n",
    "        \n",
    "        for _ in range(EPOCHS):\n",
    "            # Get random permutation of indices\n",
    "            indices = torch.randperm(len(states))\n",
    "            \n",
    "            # Mini-batch update\n",
    "            for start_idx in range(0, len(states), BATCH_SIZE):\n",
    "                idx = indices[start_idx:start_idx + BATCH_SIZE]\n",
    "                \n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_log_probs = old_log_probs[idx]\n",
    "                batch_returns = returns[idx]\n",
    "                batch_advantages = advantages[idx]\n",
    "                \n",
    "                # Evaluate actions\n",
    "                log_probs, values, entropy = self.policy.evaluate_actions(batch_states, batch_actions)\n",
    "                \n",
    "                # Policy loss\n",
    "                ratio = torch.exp(log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - CLIP_EPSILON, 1.0 + CLIP_EPSILON) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss\n",
    "                value_loss = 0.5 * ((values - batch_returns) ** 2).mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy\n",
    "                \n",
    "                # Optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy.parameters(), MAX_GRAD_NORM)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                policy_loss_epoch += policy_loss.item()\n",
    "                value_loss_epoch += value_loss.item()\n",
    "                entropy_epoch += entropy.item()\n",
    "            \n",
    "        # Update metrics\n",
    "        self.policy_losses.append(policy_loss_epoch / EPOCHS)\n",
    "        self.value_losses.append(value_loss_epoch / EPOCHS)\n",
    "        self.entropies.append(entropy_epoch / EPOCHS)\n",
    "        \n",
    "        self.training_step += 1\n",
    "        \n",
    "        return policy_loss_epoch / EPOCHS, value_loss_epoch / EPOCHS, entropy_epoch / EPOCHS\n",
    "    \n",
    "    def train(self, num_episodes, update_freq=UPDATE_FREQUENCY, visualize=False, real_time=True):\n",
    "        \"\"\"Train the agent\"\"\"\n",
    "        self.running_rewards = collections.deque(maxlen=100)\n",
    "        self.running_lengths = collections.deque(maxlen=100)\n",
    "        \n",
    "        total_steps = 0\n",
    "        \n",
    "        # Visualization setup\n",
    "        if visualize:\n",
    "            plt.ion()  # Turn on interactive mode\n",
    "            fig = plt.figure(figsize=(18, 10))\n",
    "            gs = GridSpec(2, 3, figure=fig)\n",
    "            \n",
    "            # Main environment display\n",
    "            ax_env = fig.add_subplot(gs[:, 0])\n",
    "            ax_env.set_xlim(0, TANK_SIZE)\n",
    "            ax_env.set_ylim(0, TANK_SIZE)\n",
    "            ax_env.set_aspect('equal')\n",
    "            ax_env.set_title('Fish Environment')\n",
    "            \n",
    "            # Plot center target\n",
    "            center_circle = plt.Circle(CENTER_POINT, 3, color='r', alpha=0.3)\n",
    "            ax_env.add_artist(center_circle)\n",
    "            \n",
    "            # Plot fish and currents\n",
    "            fish_plot, = ax_env.plot([], [], 'bo', markersize=8)\n",
    "            current_plots = []\n",
    "            quiver_plots = []\n",
    "            for _ in range(CURRENT_COUNT):\n",
    "                current_plots.append(plt.Circle((0, 0), CURRENT_RADIUS, color='gray', alpha=0.2))\n",
    "                quiver_plots.append(ax_env.quiver(0, 0, 0, 0, color='gray', scale=20))\n",
    "                ax_env.add_artist(current_plots[-1])\n",
    "            \n",
    "            # Plot trajectory\n",
    "            traj_plot, = ax_env.plot([], [], 'b-', alpha=0.5)\n",
    "            \n",
    "            # Plot reward history\n",
    "            ax_reward = fig.add_subplot(gs[0, 1])\n",
    "            ax_reward.set_title('Episode Reward')\n",
    "            ax_reward.set_xlabel('Episode')\n",
    "            ax_reward.set_ylabel('Total Reward')\n",
    "            reward_plot, = ax_reward.plot([], [], 'g-')\n",
    "            \n",
    "            # Plot avg distance to center\n",
    "            ax_dist = fig.add_subplot(gs[0, 2])\n",
    "            ax_dist.set_title('Distance to Center')\n",
    "            ax_dist.set_xlabel('Step')\n",
    "            ax_dist.set_ylabel('Distance')\n",
    "            dist_plot, = ax_dist.plot([], [], 'r-')\n",
    "            \n",
    "            # Plot policy loss\n",
    "            ax_policy = fig.add_subplot(gs[1, 1])\n",
    "            ax_policy.set_title('Policy Loss')\n",
    "            ax_policy.set_xlabel('Update')\n",
    "            ax_policy.set_ylabel('Loss')\n",
    "            policy_plot, = ax_policy.plot([], [], 'b-')\n",
    "            \n",
    "            # Plot value loss\n",
    "            ax_value = fig.add_subplot(gs[1, 2])\n",
    "            ax_value.set_title('Value Loss')\n",
    "            ax_value.set_xlabel('Update')\n",
    "            ax_value.set_ylabel('Loss')\n",
    "            value_plot, = ax_value.plot([], [], 'r-')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Training loop\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            # Reset episode trajectory\n",
    "            self.env.trajectory = [self.env.fish_position.copy()]\n",
    "            \n",
    "            while not done:\n",
    "                # Select action\n",
    "                action, log_prob = self.policy.get_action(state)\n",
    "                \n",
    "                # Get value estimate\n",
    "                with torch.no_grad():\n",
    "                    _, _, value = self.policy.forward(state)\n",
    "                    value = value.numpy()[0]\n",
    "                \n",
    "                # Take step in environment\n",
    "                next_state, reward, done, _, info = self.env.step(action)\n",
    "                \n",
    "                # Store transition\n",
    "                self.store_transition(state, action, log_prob, reward, done, value)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                total_steps += 1\n",
    "                \n",
    "                # Update if memory is full\n",
    "                if total_steps % update_freq == 0:\n",
    "                    self.update()\n",
    "                \n",
    "                # Visualization\n",
    "                if visualize and (real_time or done):\n",
    "                    # Update fish position\n",
    "                    fish_plot.set_data([self.env.fish_position[0]], [self.env.fish_position[1]])\n",
    "                    \n",
    "                    # Update currents\n",
    "                    for i, current in enumerate(self.env.currents):\n",
    "                        current_plots[i].center = current.position\n",
    "                        quiver_plots[i].set_offsets([current.position[0], current.position[1]])\n",
    "                        quiver_plots[i].set_UVC(current.direction[0] * current.strength, \n",
    "                                              current.direction[1] * current.strength)\n",
    "                    \n",
    "                    # Update trajectory\n",
    "                    traj_x = [pos[0] for pos in self.env.trajectory]\n",
    "                    traj_y = [pos[1] for pos in self.env.trajectory]\n",
    "                    traj_plot.set_data(traj_x, traj_y)\n",
    "                    \n",
    "                    # Update metrics plots if we have data\n",
    "                    if self.env.episode_rewards:\n",
    "                        reward_plot.set_data(range(len(self.env.episode_rewards)), self.env.episode_rewards)\n",
    "                        ax_reward.relim()\n",
    "                        ax_reward.autoscale_view()\n",
    "                    \n",
    "                    if self.env.distance_history:\n",
    "                        steps_array = np.arange(len(self.env.distance_history))\n",
    "                        # Use moving average for smoother plot\n",
    "                        window_size = min(50, len(self.env.distance_history))\n",
    "                        if window_size > 1:\n",
    "                            smooth_dist = np.convolve(self.env.distance_history, \n",
    "                                                   np.ones(window_size)/window_size, \n",
    "                                                   mode='valid')\n",
    "                            dist_plot.set_data(steps_array[window_size-1:], smooth_dist)\n",
    "                        else:\n",
    "                            dist_plot.set_data(steps_array, self.env.distance_history)\n",
    "                        ax_dist.relim()\n",
    "                        ax_dist.autoscale_view()\n",
    "                    \n",
    "                    if self.policy_losses:\n",
    "                        policy_plot.set_data(range(len(self.policy_losses)), self.policy_losses)\n",
    "                        ax_policy.relim()\n",
    "                        ax_policy.autoscale_view()\n",
    "                        \n",
    "                        value_plot.set_data(range(len(self.value_losses)), self.value_losses)\n",
    "                        ax_value.relim()\n",
    "                        ax_value.autoscale_view()\n",
    "                    \n",
    "                    plt.draw()\n",
    "                    plt.pause(0.001)\n",
    "            \n",
    "            # Episode finished\n",
    "            self.running_rewards.append(episode_reward)\n",
    "            self.running_lengths.append(steps)\n",
    "            \n",
    "            self.avg_rewards.append(np.mean(self.running_rewards))\n",
    "            self.avg_lengths.append(np.mean(self.running_lengths))\n",
    "            \n",
    "            # Print progress\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(f\"Episode {episode+1}/{num_episodes} | \" \n",
    "                      f\"Avg Reward: {self.avg_rewards[-1]:.2f} | \"\n",
    "                      f\"Avg Length: {self.avg_lengths[-1]:.2f}\")\n",
    "        \n",
    "        if visualize:\n",
    "            plt.ioff()  # Turn off interactive mode\n",
    "        \n",
    "        return self.avg_rewards, self.avg_lengths\n",
    "\n",
    "\n",
    "def run_visualization(env, agent, episodes=5):\n",
    "    \"\"\"Run visualization of trained agent\"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Store trajectory\n",
    "        trajectory = [env.fish_position.copy()]\n",
    "        \n",
    "        while not done:\n",
    "            # Select action (deterministic for visualization)\n",
    "            action = agent.policy.get_action(state, deterministic=True)\n",
    "            if isinstance(action, tuple):\n",
    "                action = action[0]\n",
    "            \n",
    "            # Take step in environment\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Store position for trajectory\n",
    "            trajectory.append(env.fish_position.copy())\n",
    "        \n",
    "        # Plot the trajectory\n",
    "        trajectory = np.array(trajectory)\n",
    "        plt.plot(trajectory[:, 0], trajectory[:, 1], '-', label=f'Episode {episode+1}')\n",
    "        \n",
    "        # Plot start and end points\n",
    "        plt.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=10)\n",
    "        plt.plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', markersize=10)\n",
    "    \n",
    "    # Plot center point\n",
    "    plt.plot(CENTER_POINT[0], CENTER_POINT[1], 'kx', markersize=10)\n",
    "    \n",
    "    plt.xlim(0, TANK_SIZE)\n",
    "    plt.ylim(0, TANK_SIZE)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.grid(True)\n",
    "    plt.title(f'Agent Trajectories after Training')\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class RealTimeTrainer:\n",
    "    \"\"\"Real-time visualization of the learning process\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agent):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.running = False\n",
    "        self.training_thread = None\n",
    "        \n",
    "        # Initialize pygame\n",
    "        pygame.init()\n",
    "        self.screen_size = 800\n",
    "        self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
    "        pygame.display.set_caption(\"Fish RL - Real-time Learning\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.episode_rewards = []\n",
    "        self.episode_distances = []\n",
    "        self.distance_history = []\n",
    "        self.avg_reward_history = []\n",
    "        self.policy_loss_history = []\n",
    "        self.value_loss_history = []\n",
    "        \n",
    "        # For showing info on screen\n",
    "        self.font = pygame.font.SysFont('Arial', 18)\n",
    "        self.large_font = pygame.font.SysFont('Arial', 24)\n",
    "        \n",
    "        # Current episode metrics\n",
    "        self.current_episode = 0\n",
    "        self.current_step = 0\n",
    "        self.total_steps = 0\n",
    "        self.last_update_step = 0\n",
    "        \n",
    "        # Scaling for visualization\n",
    "        self.scale = self.screen_size / TANK_SIZE\n",
    "    \n",
    "    def start_training(self, num_episodes=1000, update_frequency=UPDATE_FREQUENCY):\n",
    "        \"\"\"Start training in a separate thread\"\"\"\n",
    "        if self.running:\n",
    "            return\n",
    "        \n",
    "        self.running = True\n",
    "        self.training_thread = threading.Thread(\n",
    "            target=self._training_loop,\n",
    "            args=(num_episodes, update_frequency),\n",
    "            daemon=True\n",
    "        )\n",
    "        self.training_thread.start()\n",
    "    \n",
    "    def _training_loop(self, num_episodes, update_frequency):\n",
    "        \"\"\"Main training loop running in separate thread\"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            if not self.running:\n",
    "                break\n",
    "                \n",
    "            self.current_episode = episode + 1\n",
    "            state, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            self.current_step = 0\n",
    "            \n",
    "            # Calculate average distance for this episode\n",
    "            episode_distances = []\n",
    "            \n",
    "            while not done and self.running:\n",
    "                # Select action\n",
    "                action, log_prob = self.agent.policy.get_action(state)\n",
    "                \n",
    "                # Get value estimate\n",
    "                with torch.no_grad():\n",
    "                    _, _, value = self.agent.policy.forward(state)\n",
    "                    value = value.numpy()[0]\n",
    "                \n",
    "                # Take step in environment\n",
    "                next_state, reward, done, _, info = self.env.step(action)\n",
    "                \n",
    "                # Calculate distance to center\n",
    "                distance = info['distance_to_center']\n",
    "                episode_distances.append(distance)\n",
    "                self.distance_history.append(distance)\n",
    "                \n",
    "                # Store transition\n",
    "                self.agent.store_transition(state, action, log_prob, reward, done, value)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                self.current_step += 1\n",
    "                self.total_steps += 1\n",
    "                \n",
    "                # Update policy if memory is full\n",
    "                if self.total_steps % update_frequency == 0:\n",
    "                    policy_loss, value_loss, _ = self.agent.update()\n",
    "                    self.policy_loss_history.append(policy_loss)\n",
    "                    self.value_loss_history.append(value_loss)\n",
    "                    self.last_update_step = self.total_steps\n",
    "                \n",
    "                # Short sleep to not hog CPU\n",
    "                time.sleep(0.001)\n",
    "            \n",
    "            # Episode finished\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            \n",
    "            # Calculate average distance for this episode\n",
    "            avg_distance = np.mean(episode_distances) if episode_distances else 0\n",
    "            self.episode_distances.append(avg_distance)\n",
    "            \n",
    "            # Calculate average reward over last 10 episodes\n",
    "            window_size = min(10, len(self.episode_rewards))\n",
    "            avg_reward = np.mean(self.episode_rewards[-window_size:])\n",
    "            self.avg_reward_history.append(avg_reward)\n",
    "            \n",
    "            # Print progress\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(f\"Episode {episode+1}/{num_episodes} | \" \n",
    "                      f\"Reward: {episode_reward:.2f} | \"\n",
    "                      f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                      f\"Avg Distance: {avg_distance:.2f}\")\n",
    "        \n",
    "        print(\"Training finished\")\n",
    "        self.running = False\n",
    "    \n",
    "    def run_visualization(self):\n",
    "        \"\"\"Run the real-time visualization\"\"\"\n",
    "        running = True\n",
    "        \n",
    "        # Colors\n",
    "        WHITE = (255, 255, 255)\n",
    "        BLACK = (0, 0, 0)\n",
    "        BLUE = (0, 100, 255)\n",
    "        RED = (255, 0, 0)\n",
    "        GREEN = (0, 200, 0)\n",
    "        GRAY = (200, 200, 200)\n",
    "        \n",
    "        while running:\n",
    "            # Handle events\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "                    self.running = False\n",
    "            \n",
    "            # Clear the screen\n",
    "            self.screen.fill(WHITE)\n",
    "            \n",
    "            # Draw border\n",
    "            pygame.draw.rect(self.screen, BLACK, pygame.Rect(0, 0, self.screen_size, self.screen_size), 2)\n",
    "            \n",
    "            # Draw target center\n",
    "            center_x, center_y = CENTER_POINT * self.scale\n",
    "            pygame.draw.circle(self.screen, (255, 200, 200), (center_x, center_y), 20)\n",
    "            pygame.draw.circle(self.screen, RED, (center_x, center_y), 5)\n",
    "            \n",
    "            if hasattr(self.env, 'fish_position') and self.env.fish_position is not None:\n",
    "                # Draw currents\n",
    "                for current in self.env.currents:\n",
    "                    x, y = current.position * self.scale\n",
    "                    \n",
    "                    # Draw current influence area\n",
    "                    transparency = 50  # Alpha value\n",
    "                    s = pygame.Surface((CURRENT_RADIUS * 2 * self.scale, CURRENT_RADIUS * 2 * self.scale), pygame.SRCALPHA)\n",
    "                    color = tuple(int(c * 255) for c in current.color) + (transparency,)\n",
    "                    pygame.draw.circle(s, color, (CURRENT_RADIUS * self.scale, CURRENT_RADIUS * self.scale), CURRENT_RADIUS * self.scale)\n",
    "                    self.screen.blit(s, (x - CURRENT_RADIUS * self.scale, y - CURRENT_RADIUS * self.scale))\n",
    "                    \n",
    "                    # Draw arrow showing direction and strength\n",
    "                    end_x = x + current.direction[0] * current.strength * 10\n",
    "                    end_y = y + current.direction[1] * current.strength * 10\n",
    "                    pygame.draw.line(self.screen, BLACK, (x, y), (end_x, end_y), 2)\n",
    "                    \n",
    "                    # Draw arrowhead\n",
    "                    arrow_length = 10\n",
    "                    angle = math.atan2(end_y - y, end_x - x)\n",
    "                    pygame.draw.polygon(self.screen, BLACK, [\n",
    "                        (end_x, end_y),\n",
    "                        (end_x - arrow_length * math.cos(angle - math.pi/6), end_y - arrow_length * math.sin(angle - math.pi/6)),\n",
    "                        (end_x - arrow_length * math.cos(angle + math.pi/6), end_y - arrow_length * math.sin(angle + math.pi/6))\n",
    "                    ])\n",
    "                \n",
    "                # Draw fish\n",
    "                fish_x, fish_y = self.env.fish_position * self.scale\n",
    "                fish_size = 12\n",
    "                \n",
    "                # Draw fish body as a circle\n",
    "                pygame.draw.circle(self.screen, BLUE, (fish_x, fish_y), fish_size)\n",
    "                \n",
    "                # Draw fish \"face\" showing direction of motion\n",
    "                if np.linalg.norm(self.env.fish_velocity) > 0.1:\n",
    "                    direction = self.env.fish_velocity / np.linalg.norm(self.env.fish_velocity)\n",
    "                    eye_x = fish_x + direction[0] * fish_size * 0.7\n",
    "                    eye_y = fish_y + direction[1] * fish_size * 0.7\n",
    "                    pygame.draw.circle(self.screen, WHITE, (eye_x, eye_y), fish_size * 0.4)\n",
    "                \n",
    "                # Draw trajectory as a fading line\n",
    "                if len(self.env.trajectory) > 2:\n",
    "                    traj_points = [(p[0] * self.scale, p[1] * self.scale) for p in self.env.trajectory]\n",
    "                    \n",
    "                    # Use alpha to create a fading effect for older trajectory points\n",
    "                    max_points = min(50, len(traj_points))\n",
    "                    for i in range(1, max_points):\n",
    "                        alpha = int(255 * (i / max_points))\n",
    "                        pygame.draw.line(\n",
    "                            self.screen, \n",
    "                            (*BLUE[:3], alpha), \n",
    "                            traj_points[-i], \n",
    "                            traj_points[-i-1], \n",
    "                            2\n",
    "                        )\n",
    "            \n",
    "            # Draw metrics on screen\n",
    "            # 1. Episode info\n",
    "            episode_text = self.large_font.render(f\"Episode: {self.current_episode} Step: {self.current_step}\", True, BLACK)\n",
    "            self.screen.blit(episode_text, (10, 10))\n",
    "            \n",
    "            # 2. Total steps and updates\n",
    "            steps_text = self.font.render(f\"Total Steps: {self.total_steps} Last Update: {self.last_update_step}\", True, BLACK)\n",
    "            self.screen.blit(steps_text, (10, 40))\n",
    "            \n",
    "            # 3. Current distance to center if available\n",
    "            if hasattr(self.env, 'fish_position') and self.env.fish_position is not None:\n",
    "                distance = np.linalg.norm(self.env.fish_position - CENTER_POINT)\n",
    "                dist_text = self.font.render(f\"Distance to Center: {distance:.2f}\", True, BLACK)\n",
    "                self.screen.blit(dist_text, (10, 70))\n",
    "            \n",
    "            # 4. Average reward (last 10 episodes)\n",
    "            if self.avg_reward_history:\n",
    "                reward_text = self.font.render(f\"Avg Reward (10 ep): {self.avg_reward_history[-1]:.2f}\", True, BLACK)\n",
    "                self.screen.blit(reward_text, (10, 100))\n",
    "            \n",
    "            # 5. Policy loss\n",
    "            if self.policy_loss_history:\n",
    "                policy_text = self.font.render(f\"Policy Loss: {self.policy_loss_history[-1]:.4f}\", True, BLACK)\n",
    "                self.screen.blit(policy_text, (10, 130))\n",
    "            \n",
    "            # Draw metrics graphs if we have data\n",
    "            margin = 20\n",
    "            graph_width = 350\n",
    "            graph_height = 150\n",
    "            \n",
    "            if len(self.episode_rewards) > 1:\n",
    "                # Draw reward history graph\n",
    "                self._draw_graph(\n",
    "                    self.screen,\n",
    "                    self.episode_rewards,\n",
    "                    \"Episode Rewards\",\n",
    "                    self.screen_size - graph_width - margin,\n",
    "                    margin,\n",
    "                    graph_width,\n",
    "                    graph_height,\n",
    "                    GREEN\n",
    "                )\n",
    "            \n",
    "            if len(self.episode_distances) > 1:\n",
    "                # Draw distance history graph\n",
    "                self._draw_graph(\n",
    "                    self.screen,\n",
    "                    self.episode_distances,\n",
    "                    \"Avg Distance to Center\",\n",
    "                    self.screen_size - graph_width - margin,\n",
    "                    margin * 2 + graph_height,\n",
    "                    graph_width,\n",
    "                    graph_height,\n",
    "                    RED,\n",
    "                    lower_is_better=True\n",
    "                )\n",
    "            \n",
    "            if len(self.policy_loss_history) > 1:\n",
    "                # Draw policy loss graph\n",
    "                self._draw_graph(\n",
    "                    self.screen,\n",
    "                    self.policy_loss_history,\n",
    "                    \"Policy Loss\",\n",
    "                    self.screen_size - graph_width - margin,\n",
    "                    margin * 3 + graph_height * 2,\n",
    "                    graph_width,\n",
    "                    graph_height,\n",
    "                    BLUE,\n",
    "                    lower_is_better=True\n",
    "                )\n",
    "            \n",
    "            # Update display\n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(60)  # 60 FPS\n",
    "        \n",
    "        pygame.quit()\n",
    "    \n",
    "    def _draw_graph(self, surface, data, title, x, y, width, height, color, lower_is_better=False):\n",
    "        \"\"\"Draw a simple line graph on the surface\"\"\"\n",
    "        # Colors\n",
    "        WHITE = (255, 255, 255)\n",
    "        BLACK = (0, 0, 0)\n",
    "        GRAY = (200, 200, 200)\n",
    "        \n",
    "        # Draw background\n",
    "        pygame.draw.rect(surface, WHITE, pygame.Rect(x, y, width, height))\n",
    "        pygame.draw.rect(surface, BLACK, pygame.Rect(x, y, width, height), 1)\n",
    "        \n",
    "        # Draw title\n",
    "        title_text = self.font.render(title, True, BLACK)\n",
    "        surface.blit(title_text, (x + 5, y + 5))\n",
    "        \n",
    "        # Get data for graph\n",
    "        if len(data) <= 1:\n",
    "            return\n",
    "            \n",
    "        # Draw axes\n",
    "        pygame.draw.line(surface, GRAY, (x + 30, y + 30), (x + 30, y + height - 10), 1)  # Y-axis\n",
    "        pygame.draw.line(surface, GRAY, (x + 30, y + height - 10), (x + width - 10, y + height - 10), 1)  # X-axis\n",
    "        \n",
    "        # Use only last 100 points for better visualization\n",
    "        if len(data) > 100:\n",
    "            plot_data = data[-100:]\n",
    "        else:\n",
    "            plot_data = data\n",
    "        \n",
    "        # Calculate min/max values\n",
    "        data_min = min(plot_data)\n",
    "        data_max = max(plot_data)\n",
    "        data_range = data_max - data_min\n",
    "        \n",
    "        # To avoid division by zero\n",
    "        if data_range == 0:\n",
    "            data_range = 1\n",
    "        \n",
    "        # Calculate scale\n",
    "        x_scale = (width - 50) / len(plot_data)\n",
    "        y_scale = (height - 50) / data_range\n",
    "        \n",
    "        # Calculate points\n",
    "        points = []\n",
    "        for i, value in enumerate(plot_data):\n",
    "            point_x = x + 30 + i * x_scale\n",
    "            # Invert Y if lower is better\n",
    "            if lower_is_better:\n",
    "                point_y = y + height - 10 - (value - data_min) * y_scale\n",
    "            else:\n",
    "                point_y = y + height - 10 - (value - data_min) * y_scale\n",
    "            points.append((point_x, point_y))\n",
    "        \n",
    "        # Draw line\n",
    "        if len(points) > 1:\n",
    "            pygame.draw.lines(surface, color, False, points, 2)\n",
    "        \n",
    "        # Draw min/max values\n",
    "        min_text = self.font.render(f\"{data_min:.2f}\", True, BLACK)\n",
    "        max_text = self.font.render(f\"{data_max:.2f}\", True, BLACK)\n",
    "        surface.blit(min_text, (x + 5, y + height - 25))\n",
    "        surface.blit(max_text, (x + 5, y + 35))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the fish RL training\"\"\"\n",
    "    # Create environment\n",
    "    env = FishEnv()\n",
    "    \n",
    "    # Create agent\n",
    "    agent = PPOAgent(env)\n",
    "    \n",
    "    # Create real-time trainer\n",
    "    trainer = RealTimeTrainer(env, agent)\n",
    "    \n",
    "    # Start training in background thread\n",
    "    trainer.start_training(num_episodes=1000)\n",
    "    \n",
    "    # Run visualization\n",
    "    trainer.run_visualization()\n",
    "    \n",
    "    # After visualization is closed, we can save the trained model\n",
    "    torch.save(agent.policy.state_dict(), \"fish_policy.pt\")\n",
    "    \n",
    "    # Run a few episodes with the trained agent\n",
    "    run_visualization(env, agent)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ff5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Selfcontained Fish RL script (gymstyle env + PPO + realtime pygame viewer).\n",
    "Drop this file in JupyterLab or run via:  python fish_rl_full.py\n",
    "Requires: numpy, gymnasium, pygame, matplotlib, torch.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import threading\n",
    "import collections\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# \n",
    "# Constants & Hyperparameters\n",
    "# \n",
    "TANK_SIZE               = 100\n",
    "CENTER_POINT            = np.array([TANK_SIZE / 2, TANK_SIZE / 2])\n",
    "MAX_VELOCITY            = 30\n",
    "MAX_FORCE               = 10\n",
    "\n",
    "CURRENT_COUNT           = 3\n",
    "MAX_CURRENT_STRENGTH    = 3.0\n",
    "CURRENT_CHANGE_PROB     = 0.01\n",
    "CURRENT_RADIUS          = 30.0\n",
    "WATER_RESISTANCE        = 0.1\n",
    "TIMESTEP                = 0.1\n",
    "\n",
    "LEARNING_RATE           = 3e-4\n",
    "GAMMA                   = 0.99\n",
    "GAE_LAMBDA              = 0.95\n",
    "CLIP_EPSILON            = 0.2\n",
    "VALUE_COEF              = 0.5\n",
    "ENTROPY_COEF            = 0.005\n",
    "MAX_GRAD_NORM           = 0.5\n",
    "BATCH_SIZE              = 256\n",
    "BUFFER_SIZE             = 1024\n",
    "EPOCHS                  = 4\n",
    "UPDATE_FREQUENCY        = 1024\n",
    "\n",
    "# Seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# \n",
    "# Environment pieces\n",
    "# \n",
    "class WaterCurrent:\n",
    "    \"\"\"Simple moving current.\"\"\"\n",
    "    def __init__(self, tank_size: float):\n",
    "        self.tank_size = tank_size\n",
    "        self.position  = np.random.uniform(0, tank_size, 2)\n",
    "        self.direction = self._rand_dir()\n",
    "        self.strength  = np.random.uniform(1.0, MAX_CURRENT_STRENGTH)\n",
    "        self.color     = np.random.uniform(0, 1, 3)\n",
    "        self.velocity  = np.random.uniform(1.0, 3.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _rand_dir():\n",
    "        ang = np.random.uniform(0, 2 * np.pi)\n",
    "        return np.array([math.cos(ang), math.sin(ang)])\n",
    "\n",
    "    def update(self):\n",
    "        self.position += self.direction * self.velocity * TIMESTEP\n",
    "        for i in range(2):\n",
    "            if self.position[i] < 0:\n",
    "                self.position[i] = 0; self.direction[i] *= -1\n",
    "            elif self.position[i] > self.tank_size:\n",
    "                self.position[i] = self.tank_size; self.direction[i] *= -1\n",
    "        if np.random.rand() < CURRENT_CHANGE_PROB:\n",
    "            self.direction = self._rand_dir()\n",
    "            self.strength  = np.random.uniform(1.0, MAX_CURRENT_STRENGTH)\n",
    "\n",
    "    def get_force_at(self, pos: np.ndarray) -> np.ndarray:\n",
    "        dist = np.linalg.norm(pos - self.position)\n",
    "        if dist > CURRENT_RADIUS:\n",
    "            return np.zeros(2)\n",
    "        mag = self.strength * (1 - dist / CURRENT_RADIUS)\n",
    "        return mag * self.direction\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "class FishEnv(gym.Env):\n",
    "    \"\"\"2D fish that tries to reach tank center\"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"rgb_array\"]}\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Normalised observation (all values [1,1] or [0,1])\n",
    "        low  = np.array([0, 0,           # pos 01\n",
    "                         -1, -1] +       # vel 1..1\n",
    "                         ([0, 0, -1, -1, 0] * CURRENT_COUNT), dtype=np.float32)\n",
    "        high = np.array([1, 1,\n",
    "                         1, 1] +\n",
    "                         ([1, 1, 1, 1, 1] * CURRENT_COUNT), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "        self.action_space      = spaces.Box(-MAX_FORCE, MAX_FORCE, (2,), np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.max_steps   = 1000\n",
    "        self.reset()\n",
    "\n",
    "        # pygame helpers\n",
    "        self._screen = None\n",
    "        self._clock  = None\n",
    "\n",
    "    # ----- Helpers -----------------------------------------------------------\n",
    "    def _obs(self) -> np.ndarray:\n",
    "        pos = self.fish_pos / TANK_SIZE\n",
    "        vel = self.fish_vel / MAX_VELOCITY\n",
    "        parts = [pos, vel]\n",
    "        for cur in self.currents:\n",
    "            parts.extend([\n",
    "                cur.position / TANK_SIZE,\n",
    "                cur.direction,\n",
    "                np.array([cur.strength / MAX_CURRENT_STRENGTH])\n",
    "            ])\n",
    "        return np.concatenate(parts).astype(np.float32)\n",
    "\n",
    "    def _info(self) -> Dict:\n",
    "        return {\"distance_to_center\": float(np.linalg.norm(self.fish_pos - CENTER_POINT)),\n",
    "                \"step\": self.steps}\n",
    "\n",
    "    # ----- Gym API -----------------------------------------------------------\n",
    "    def reset(self, *, seed: Optional[int] = None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # spawn away from centre\n",
    "        while True:\n",
    "            self.fish_pos = np.random.uniform(0, TANK_SIZE, 2)\n",
    "            if np.linalg.norm(self.fish_pos - CENTER_POINT) > 20:\n",
    "                break\n",
    "        self.fish_vel = np.zeros(2, dtype=float)\n",
    "        self.currents = [WaterCurrent(TANK_SIZE) for _ in range(CURRENT_COUNT)]\n",
    "        self.steps    = 0\n",
    "        self.trajectory = [self.fish_pos.copy()]\n",
    "        return self._obs(), self._info()\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        self.steps += 1\n",
    "        # env **does not** clip; assume actor sent inrange values\n",
    "        thrust = np.clip(action, -MAX_FORCE, MAX_FORCE)\n",
    "\n",
    "        # water forces\n",
    "        cur_force = np.zeros(2)\n",
    "        for cur in self.currents:\n",
    "            cur.update(); cur_force += cur.get_force_at(self.fish_pos)\n",
    "\n",
    "        water_drag  = -WATER_RESISTANCE * self.fish_vel * np.abs(self.fish_vel)\n",
    "        total_force = thrust + cur_force + water_drag\n",
    "\n",
    "        # integrate\n",
    "        self.fish_vel += total_force * TIMESTEP\n",
    "        self.fish_vel  = np.clip(self.fish_vel, -MAX_VELOCITY, MAX_VELOCITY)\n",
    "        self.fish_pos += self.fish_vel * TIMESTEP\n",
    "\n",
    "        # bounce walls\n",
    "        for i in range(2):\n",
    "            if self.fish_pos[i] < 0:\n",
    "                self.fish_pos[i] = 0; self.fish_vel[i] *= -0.5\n",
    "            elif self.fish_pos[i] > TANK_SIZE:\n",
    "                self.fish_pos[i] = TANK_SIZE; self.fish_vel[i] *= -0.5\n",
    "\n",
    "        self.trajectory.append(self.fish_pos.copy())\n",
    "\n",
    "        # ----- reward --------------------------------------------------------\n",
    "        dist = np.linalg.norm(self.fish_pos - CENTER_POINT)\n",
    "        position_reward = 10 * (math.exp(-0.05 * dist) - 1)   # 0 at centre, <0 elsewhere\n",
    "        living_bonus    = 0.1\n",
    "        vel_pen         = -0.01 * np.linalg.norm(self.fish_vel)\n",
    "        thrust_pen      = -0.005 * np.linalg.norm(thrust)\n",
    "        reward          = position_reward + living_bonus + vel_pen + thrust_pen\n",
    "\n",
    "        done = self.steps >= self.max_steps\n",
    "        truncated = False\n",
    "        return self._obs(), float(reward), done, truncated, self._info()\n",
    "\n",
    "    # ----- Render ------------------------------------------------------------\n",
    "    def render(self):\n",
    "        if self.render_mode != \"rgb_array\":\n",
    "            return\n",
    "        if self._screen is None:\n",
    "            pygame.init(); pygame.display.init()\n",
    "            self._screen = pygame.Surface((TANK_SIZE*7, TANK_SIZE*7))\n",
    "            self._clock  = pygame.time.Clock()\n",
    "        surf = self._screen\n",
    "        surf.fill((255, 255, 255))\n",
    "        scl = 7\n",
    "        pygame.draw.rect(surf, (0,0,0), pygame.Rect(0,0,TANK_SIZE*scl,TANK_SIZE*scl), 2)\n",
    "        pygame.draw.circle(surf, (255,200,200), (CENTER_POINT[0]*scl, CENTER_POINT[1]*scl), 10)\n",
    "        for cur in self.currents:\n",
    "            x,y = cur.position*scl\n",
    "            s   = pygame.Surface((CURRENT_RADIUS*2*scl, CURRENT_RADIUS*2*scl), pygame.SRCALPHA)\n",
    "            color = tuple(int(c*255) for c in cur.color)+(50,)\n",
    "            pygame.draw.circle(s, color, (CURRENT_RADIUS*scl, CURRENT_RADIUS*scl), CURRENT_RADIUS*scl)\n",
    "            surf.blit(s,(x-CURRENT_RADIUS*scl, y-CURRENT_RADIUS*scl))\n",
    "            end = (x+cur.direction[0]*cur.strength*5, y+cur.direction[1]*cur.strength*5)\n",
    "            pygame.draw.line(surf,(0,0,0),(x,y),end,2)\n",
    "        fx,fy = self.fish_pos*scl\n",
    "        pygame.draw.circle(surf,(0,100,255),(fx,fy),6)\n",
    "        arr = np.transpose(np.array(pygame.surfarray.pixels3d(surf)), (1,0,2))\n",
    "        return arr\n",
    "\n",
    "# \n",
    "# PPO Agent\n",
    "# \n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, obs_size:int, act_size:int):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_size,128), nn.ReLU(),\n",
    "            nn.Linear(128,128), nn.ReLU())\n",
    "        self.mean = nn.Linear(128, act_size)\n",
    "        self.log_std = nn.Parameter(torch.ones(act_size) * math.log(3.0))\n",
    "        self.value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x).float()\n",
    "        h = self.shared(x)\n",
    "        return self.mean(h), torch.exp(self.log_std), self.value(h).squeeze(-1)\n",
    "\n",
    "    # sample + logprob w.r.t Normal BEFORE clipping\n",
    "    def act(self, state: np.ndarray):\n",
    "        with torch.no_grad():\n",
    "            mu, std, val = self(state)\n",
    "            dist = Normal(mu, std)\n",
    "            a = dist.sample()\n",
    "            logp = dist.log_prob(a).sum(-1)\n",
    "            a_clipped = torch.clamp(a, -MAX_FORCE, MAX_FORCE)\n",
    "        return a_clipped.numpy(), logp.numpy(), val.item()\n",
    "\n",
    "    def evaluate(self, states, actions):\n",
    "        mu, std, values = self(states)\n",
    "        dist = Normal(mu, std)\n",
    "        logp = dist.log_prob(actions).sum(-1)\n",
    "        entropy = dist.entropy().sum(-1)\n",
    "        return logp, values, entropy\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "class PPO:\n",
    "    def __init__(self, env: FishEnv):\n",
    "        self.env = env\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.shape[0]\n",
    "        self.net = PolicyNet(obs_dim, act_dim)\n",
    "        self.optim = optim.Adam(self.net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = []\n",
    "        self.steps = 0\n",
    "        # stats\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses  = []\n",
    "\n",
    "    def store(self, *args):\n",
    "        self.memory.append(args)  # state, act, logp, rew, done, val\n",
    "\n",
    "    def _finish_path(self):\n",
    "        states,acts,logps,rews,dones,vals = zip(*self.memory)\n",
    "        self.memory = []\n",
    "        states = np.array(states)\n",
    "        acts   = np.array(acts)\n",
    "        logps  = np.array(logps)\n",
    "        vals   = np.array(vals)\n",
    "        rets   = np.zeros_like(rews, dtype=float)\n",
    "        advs   = np.zeros_like(rews, dtype=float)\n",
    "        next_val = 0\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rews))):\n",
    "            mask = 1.0 - dones[t]\n",
    "            delta = rews[t] + GAMMA*next_val*mask - vals[t]\n",
    "            gae   = delta + GAMMA*GAE_LAMBDA*mask*gae\n",
    "            advs[t] = gae\n",
    "            rets[t] = gae + vals[t]\n",
    "            next_val = vals[t]\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        return states, acts, logps, rets, advs\n",
    "\n",
    "    def update(self):\n",
    "        states, actions, old_logp, returns, advantages = self._finish_path()\n",
    "        states  = torch.from_numpy(states).float()\n",
    "        actions = torch.from_numpy(actions).float()\n",
    "        old_logp= torch.from_numpy(old_logp).float()\n",
    "        returns = torch.from_numpy(returns).float()\n",
    "        advantages = torch.from_numpy(advantages).float()\n",
    "        pol_loss_epoch = 0; val_loss_epoch = 0; ent_epoch = 0\n",
    "        for _ in range(EPOCHS):\n",
    "            idx = torch.randperm(len(states))\n",
    "            for start in range(0, len(states), BATCH_SIZE):\n",
    "                j = idx[start:start+BATCH_SIZE]\n",
    "                logp, vals, ent = self.net.evaluate(states[j], actions[j])\n",
    "                ratio = torch.exp(logp - old_logp[j])\n",
    "                surr1 = ratio * advantages[j]\n",
    "                surr2 = torch.clamp(ratio, 1-CLIP_EPSILON, 1+CLIP_EPSILON) * advantages[j]\n",
    "                pol_loss = -torch.min(surr1,surr2).mean()\n",
    "                val_loss = 0.5 * (returns[j] - vals).pow(2).mean()\n",
    "                loss = pol_loss + VALUE_COEF*val_loss - ENTROPY_COEF*ent.mean()\n",
    "                self.optim.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(self.net.parameters(), MAX_GRAD_NORM); self.optim.step()\n",
    "                pol_loss_epoch += pol_loss.item(); val_loss_epoch += val_loss.item(); ent_epoch += ent.mean().item()\n",
    "        n_batches = EPOCHS * math.ceil(len(states)/BATCH_SIZE)\n",
    "        self.policy_losses.append(pol_loss_epoch/n_batches)\n",
    "        self.value_losses.append(val_loss_epoch/n_batches)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    def train(self, episodes=500):\n",
    "        state,_ = self.env.reset()\n",
    "        ep_reward=0; ep_len=0\n",
    "        for ep in range(episodes):\n",
    "            done=False\n",
    "            while not done:\n",
    "                act,logp,val = self.net.act(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(act)\n",
    "                self.store(state, act, logp, reward, done, val)\n",
    "                state = next_state\n",
    "                ep_reward += reward; ep_len += 1; self.steps += 1\n",
    "                if self.steps % UPDATE_FREQUENCY == 0:\n",
    "                    self.update()\n",
    "            # episode end\n",
    "            self.ep_rewards.append(ep_reward); self.ep_lengths.append(ep_len)\n",
    "            if (ep+1)%10==0:\n",
    "                avg_r = np.mean(self.ep_rewards[-10:])\n",
    "                print(f\"Episode {ep+1}  avgR={avg_r:.1f}  len={np.mean(self.ep_lengths[-10:]):.0f}\")\n",
    "            state,_ = self.env.reset(); ep_reward=0; ep_len=0\n",
    "        # final update if leftovers\n",
    "        if self.memory:\n",
    "            self.update()\n",
    "\n",
    "# \n",
    "# Simple demo when run standalone\n",
    "# \n",
    "\n",
    "def visualize_traj(env: FishEnv, agent: PPO, episodes: int = 5):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    for ep in range(episodes):\n",
    "        s,_ = env.reset(); done=False; traj=[env.fish_pos.copy()]\n",
    "        while not done:\n",
    "            a,_ ,_ = agent.net.act(s)\n",
    "            s,_,done,_,_ = env.step(a); traj.append(env.fish_pos.copy())\n",
    "        traj = np.array(traj)\n",
    "        plt.plot(traj[:,0], traj[:,1], label=f\"ep{ep+1}\")\n",
    "        plt.plot(traj[0,0], traj[0,1], 'go'); plt.plot(traj[-1,0], traj[-1,1], 'ro')\n",
    "    plt.scatter(CENTER_POINT[0], CENTER_POINT[1], c='k', marker='x');\n",
    "    plt.xlim(0,TANK_SIZE); plt.ylim(0,TANK_SIZE); plt.gca().set_aspect('equal');\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "env = FishEnv()\n",
    "# disable currents for quick smoketest\n",
    "# env.currents = []\n",
    "agent = PPO(env)\n",
    "agent.train(episodes=300)\n",
    "visualize_traj(env, agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.animation as animation\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import threading\n",
    "import time\n",
    "from typing import Dict, Tuple, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import os\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24637c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Constants\n",
    "TANK_SIZE = 10.0  # Size of the water tank (10x10 units)\n",
    "CENTER_POINT = np.array([TANK_SIZE/2, TANK_SIZE/2])  # Center of the tank\n",
    "MAX_EPISODE_STEPS = 1000  # Maximum steps per episode\n",
    "VISUALIZATION_INTERVAL = 10  # Update visualization every 10 steps\n",
    "MAX_TRAINING_EPISODES = 500  # Number of episodes to train for\n",
    "\n",
    "# Colors\n",
    "WATER_COLOR = '#AADDFF'\n",
    "FISH_COLOR = '#FF7F50'\n",
    "CURRENT_COLORS = ['#0000FF33', '#0000AA44', '#00007755']  # Transparent blues for currents\n",
    "\n",
    "class WaterCurrent:\n",
    "    \"\"\"Represents a dynamic water current in the tank\"\"\"\n",
    "    def __init__(self, tank_size, strength_range=(0.05, 0.2), radius_range=(1.0, 3.0)):\n",
    "        self.tank_size = tank_size\n",
    "        self.strength_range = strength_range\n",
    "        self.radius_range = radius_range\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        # Random position within the tank\n",
    "        self.position = np.random.uniform(0, self.tank_size, size=2)\n",
    "        # Random direction vector (normalized)\n",
    "        angle = np.random.uniform(0, 2*np.pi)\n",
    "        self.direction = np.array([np.cos(angle), np.sin(angle)])\n",
    "        # Random strength and radius of influence\n",
    "        self.strength = np.random.uniform(*self.strength_range)\n",
    "        self.radius = np.random.uniform(*self.radius_range)\n",
    "        # Movement parameters\n",
    "        self.velocity = np.random.uniform(0.01, 0.05, size=2)\n",
    "        self.turn_prob = 0.02  # Probability to change direction each step\n",
    "    \n",
    "    def update(self):\n",
    "        # Randomly change direction with low probability\n",
    "        if np.random.random() < self.turn_prob:\n",
    "            angle = np.random.uniform(0, 2*np.pi)\n",
    "            self.direction = np.array([np.cos(angle), np.sin(angle)])\n",
    "            # Also potentially change strength\n",
    "            self.strength = np.random.uniform(*self.strength_range)\n",
    "        \n",
    "        # Move current\n",
    "        self.position += self.velocity * self.direction\n",
    "        \n",
    "        # Bounce off walls\n",
    "        for i in range(2):\n",
    "            if self.position[i] < 0:\n",
    "                self.position[i] = 0\n",
    "                self.direction[i] *= -1\n",
    "            elif self.position[i] > self.tank_size:\n",
    "                self.position[i] = self.tank_size\n",
    "                self.direction[i] *= -1\n",
    "    \n",
    "    def get_force(self, fish_position):\n",
    "        \"\"\"Calculate the force applied to the fish based on distance\"\"\"\n",
    "        dist_vector = self.position - fish_position\n",
    "        distance = np.linalg.norm(dist_vector)\n",
    "        \n",
    "        # If fish is outside radius of influence, no force is applied\n",
    "        if distance > self.radius:\n",
    "            return np.zeros(2)\n",
    "        \n",
    "        # Force decreases with distance from current center (inverse linear)\n",
    "        force_magnitude = self.strength * (1 - distance/self.radius)\n",
    "        force = force_magnitude * self.direction\n",
    "        \n",
    "        return force\n",
    "\n",
    "class FishTankEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for fish navigation task\"\"\"\n",
    "    metadata = {'render_modes': ['human']}\n",
    "    \n",
    "    def __init__(self, num_currents=3, drag_coefficient=0.1, wall_repulsion=2.0):\n",
    "        super(FishTankEnv, self).__init__()\n",
    "        \n",
    "        # Action space: continuous thrust in x and y direction [-1.0, 1.0]\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(2,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Observation space: [x, y, vx, vy, cx1, cy1, cs1, cr1, ..., cxn, cyn, csn, crn]\n",
    "        # where (cx, cy) is current position, cs is current strength, cr is current radius\n",
    "        obs_size = 4 + 4 * num_currents  # Fish state + currents state\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-float('inf'), high=float('inf'), shape=(obs_size,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Environment parameters\n",
    "        self.tank_size = TANK_SIZE\n",
    "        self.center = np.array([self.tank_size/2, self.tank_size/2])\n",
    "        self.max_thrust = 0.5  # Maximum thrust force\n",
    "        self.drag_coefficient = drag_coefficient\n",
    "        self.wall_repulsion = wall_repulsion\n",
    "        self.dt = 0.1  # Time step\n",
    "        \n",
    "        # Currents\n",
    "        self.num_currents = num_currents\n",
    "        self.currents = [WaterCurrent(self.tank_size) for _ in range(num_currents)]\n",
    "        \n",
    "        # Fish state\n",
    "        self.position = None\n",
    "        self.velocity = None\n",
    "        self.steps = 0\n",
    "        \n",
    "        # For visualization\n",
    "        self.trajectory = []\n",
    "        self.reset()\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Create observation vector\"\"\"\n",
    "        obs = np.concatenate([\n",
    "            self.position,\n",
    "            self.velocity\n",
    "        ])\n",
    "        \n",
    "        # Add current information\n",
    "        for current in self.currents:\n",
    "            current_info = np.concatenate([\n",
    "                current.position,\n",
    "                [current.strength],\n",
    "                [current.radius]\n",
    "            ])\n",
    "            obs = np.concatenate([obs, current_info])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset fish position (start at random position)\n",
    "        self.position = np.random.uniform(1.0, self.tank_size-1.0, size=2)\n",
    "        self.velocity = np.zeros(2)\n",
    "        self.steps = 0\n",
    "        self.trajectory = [self.position.copy()]\n",
    "        \n",
    "        # Reset currents\n",
    "        for current in self.currents:\n",
    "            current.reset()\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment given an action\"\"\"\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Apply the action (thrust force)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        thrust = action * self.max_thrust\n",
    "        \n",
    "        # Apply water currents\n",
    "        current_force = np.zeros(2)\n",
    "        for current in self.currents:\n",
    "            current.update()\n",
    "            current_force += current.get_force(self.position)\n",
    "        \n",
    "        # Apply drag force (proportional to velocity)\n",
    "        drag_force = -self.drag_coefficient * self.velocity\n",
    "        \n",
    "        # Apply wall repulsion force to prevent sticking to walls\n",
    "        wall_force = np.zeros(2)\n",
    "        \n",
    "        # X-direction wall forces\n",
    "        if self.position[0] < 1.0:\n",
    "            wall_force[0] = self.wall_repulsion * (1.0 - self.position[0])\n",
    "        elif self.position[0] > self.tank_size - 1.0:\n",
    "            wall_force[0] = self.wall_repulsion * (self.tank_size - 1.0 - self.position[0])\n",
    "            \n",
    "        # Y-direction wall forces\n",
    "        if self.position[1] < 1.0:\n",
    "            wall_force[1] = self.wall_repulsion * (1.0 - self.position[1])\n",
    "        elif self.position[1] > self.tank_size - 1.0:\n",
    "            wall_force[1] = self.wall_repulsion * (self.tank_size - 1.0 - self.position[1])\n",
    "        \n",
    "        # Total force\n",
    "        total_force = thrust + current_force + drag_force + wall_force\n",
    "        \n",
    "        # Update velocity and position using Euler integration\n",
    "        self.velocity += total_force * self.dt\n",
    "        self.position += self.velocity * self.dt\n",
    "        \n",
    "        # Ensure fish stays within bounds\n",
    "        for i in range(2):\n",
    "            if self.position[i] < 0:\n",
    "                self.position[i] = 0\n",
    "                self.velocity[i] = 0  # Stop at the wall\n",
    "            elif self.position[i] > self.tank_size:\n",
    "                self.position[i] = self.tank_size\n",
    "                self.velocity[i] = 0  # Stop at the wall\n",
    "        \n",
    "        # Add to trajectory\n",
    "        self.trajectory.append(self.position.copy())\n",
    "        \n",
    "        # Calculate reward\n",
    "        distance_to_center = np.linalg.norm(self.position - self.center)\n",
    "        max_distance = np.sqrt(2) * self.tank_size / 2  # Maximum possible distance\n",
    "        \n",
    "        # Exponential reward for being close to center (high sensitivity near center)\n",
    "        # Normalized distance in [0, 1]\n",
    "        normalized_distance = distance_to_center / max_distance\n",
    "        center_reward = np.exp(-5 * normalized_distance) - 0.1  # Higher reward near center\n",
    "        \n",
    "        # Penalty for high speed\n",
    "        velocity_penalty = -0.01 * np.linalg.norm(self.velocity)**2\n",
    "        \n",
    "        # Penalty for high thrust (encourage efficiency)\n",
    "        thrust_penalty = -0.05 * np.linalg.norm(thrust)**2\n",
    "        \n",
    "        # Reward for counteracting currents effectively\n",
    "        current_reward = 0.0\n",
    "        if np.linalg.norm(current_force) > 0:\n",
    "            # Reward if thrust direction opposes current direction\n",
    "            current_direction = current_force / np.linalg.norm(current_force)\n",
    "            thrust_direction = thrust / (np.linalg.norm(thrust) + 1e-6)\n",
    "            alignment = -np.dot(current_direction, thrust_direction)\n",
    "            if alignment > 0:\n",
    "                current_reward = 0.1 * alignment * np.linalg.norm(current_force)\n",
    "        \n",
    "        # Total reward\n",
    "        reward = center_reward + velocity_penalty + thrust_penalty + current_reward\n",
    "        \n",
    "        # Check if episode is done\n",
    "        terminated = self.steps >= MAX_EPISODE_STEPS\n",
    "        truncated = False\n",
    "        \n",
    "        # Info dictionary\n",
    "        info = {\n",
    "            'distance_to_center': distance_to_center,\n",
    "            'center_reward': center_reward,\n",
    "            'velocity_penalty': velocity_penalty,\n",
    "            'thrust_penalty': thrust_penalty,\n",
    "            'current_reward': current_reward\n",
    "        }\n",
    "        \n",
    "        return self._get_observation(), reward, terminated, truncated, info\n",
    "\n",
    "# Neural network for PPO\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared network layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor network (policy)\n",
    "        self.actor_mean = nn.Linear(128, action_dim)\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(1, action_dim))\n",
    "        \n",
    "        # Critic network (value function)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        state = torch.FloatTensor(state)\n",
    "        shared_features = self.forward(state)\n",
    "        \n",
    "        action_mean = self.actor_mean(shared_features)\n",
    "        action_std = torch.exp(self.actor_log_std).expand_as(action_mean)\n",
    "        \n",
    "        if deterministic:\n",
    "            return action_mean.detach().numpy()\n",
    "        \n",
    "        dist = Normal(action_mean, action_std)\n",
    "        action = dist.sample()\n",
    "        return action.detach().numpy()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        shared_features = self.forward(state)\n",
    "        \n",
    "        # Get action distribution\n",
    "        action_mean = self.actor_mean(shared_features)\n",
    "        action_std = torch.exp(self.actor_log_std).expand_as(action_mean)\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        \n",
    "        # Get log probability of action\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)\n",
    "        entropy = dist.entropy().sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Get state value\n",
    "        value = self.critic(shared_features)\n",
    "        \n",
    "        return log_prob, entropy, value\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2, epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.old_policy = ActorCritic(state_dim, action_dim)\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MSE_loss = nn.MSELoss()\n",
    "    \n",
    "    def update(self, memory):\n",
    "        # Convert memory to tensors\n",
    "        states = torch.FloatTensor(memory.states)\n",
    "        actions = torch.FloatTensor(memory.actions)\n",
    "        rewards = torch.FloatTensor(memory.rewards)\n",
    "        next_states = torch.FloatTensor(memory.next_states)\n",
    "        dones = torch.FloatTensor(memory.dones)\n",
    "        \n",
    "        # Calculate returns (discounted rewards)\n",
    "        returns = []\n",
    "        discounted_reward = 0\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + self.gamma * discounted_reward\n",
    "            returns.insert(0, discounted_reward)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns)\n",
    "        \n",
    "        # Normalize returns\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-7)\n",
    "        \n",
    "        # Get old log probabilities, entropies, and values\n",
    "        with torch.no_grad():\n",
    "            old_log_probs, _, old_values = self.old_policy.evaluate(states, actions)\n",
    "            old_values = old_values.detach()\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = returns - old_values\n",
    "        \n",
    "        # Optimize policy for epochs iterations\n",
    "        for _ in range(self.epochs):\n",
    "            # Evaluate current policy\n",
    "            log_probs, entropies, values = self.policy.evaluate(states, actions)\n",
    "            \n",
    "            # Calculate ratios\n",
    "            ratios = torch.exp(log_probs - old_log_probs.detach())\n",
    "            \n",
    "            # Calculate surrogate losses\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            \n",
    "            # Calculate total loss\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = self.MSE_loss(values, returns)\n",
    "            entropy_loss = -0.01 * entropies.mean()  # Encourage exploration\n",
    "            \n",
    "            loss = actor_loss + 0.5 * critic_loss + entropy_loss\n",
    "            \n",
    "            # Take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Update old policy\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "\n",
    "class FishTrainer:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        self.agent = PPO(state_dim, action_dim)\n",
    "        \n",
    "        # Training metrics\n",
    "        self.episode_rewards = []\n",
    "        self.avg_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.distances_to_center = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        \n",
    "        # For visualization\n",
    "        self.current_episode = 0\n",
    "        self.current_step = 0\n",
    "        self.is_training = False\n",
    "        self.memory = Memory()\n",
    "        \n",
    "        # Rolling window for metrics\n",
    "        self.reward_window = deque(maxlen=10)\n",
    "        \n",
    "        # For thread safety\n",
    "        self.update_lock = threading.Lock()\n",
    "        \n",
    "        # Trained model path\n",
    "        self.model_path = 'fish_model.pth'\n",
    "        \n",
    "        # Best reward so far\n",
    "        self.best_reward = -float('inf')\n",
    "    \n",
    "    def run_episode(self):\n",
    "        state, _ = self.env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_distance = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from policy\n",
    "            with self.update_lock:\n",
    "                action = self.agent.policy.get_action(state)\n",
    "            \n",
    "            # Take a step in the environment\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store transition in memory\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_distance += info['distance_to_center']\n",
    "            \n",
    "            with self.update_lock:\n",
    "                self.current_step += 1\n",
    "        \n",
    "        # Update metrics\n",
    "        with self.update_lock:\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.reward_window.append(episode_reward)\n",
    "            self.avg_rewards.append(sum(self.reward_window) / len(self.reward_window))\n",
    "            self.episode_lengths.append(self.env.steps)\n",
    "            self.distances_to_center.append(episode_distance / self.env.steps)\n",
    "            \n",
    "            # Update policy\n",
    "            policy_loss, value_loss = self.agent.update(self.memory)\n",
    "            self.policy_losses.append(policy_loss)\n",
    "            self.value_losses.append(value_loss)\n",
    "            \n",
    "            # Clear memory for next episode\n",
    "            self.memory.clear()\n",
    "            \n",
    "            # Save model if best\n",
    "            if episode_reward > self.best_reward:\n",
    "                self.best_reward = episode_reward\n",
    "                torch.save(self.agent.policy.state_dict(), self.model_path)\n",
    "            \n",
    "            self.current_episode += 1\n",
    "    \n",
    "    def train(self, num_episodes=MAX_TRAINING_EPISODES):\n",
    "        \"\"\"Train the agent for specified number of episodes\"\"\"\n",
    "        self.is_training = True\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            if not self.is_training:\n",
    "                break\n",
    "            self.run_episode()\n",
    "        \n",
    "        self.is_training = False\n",
    "        print(f\"Training completed. Model saved to {self.model_path}\")\n",
    "    \n",
    "    def start_training_thread(self, num_episodes=MAX_TRAINING_EPISODES):\n",
    "        \"\"\"Start training in a separate thread\"\"\"\n",
    "        self.training_thread = threading.Thread(target=self.train, args=(num_episodes,))\n",
    "        self.training_thread.daemon = True\n",
    "        self.training_thread.start()\n",
    "    \n",
    "    def stop_training(self):\n",
    "        \"\"\"Stop the training process\"\"\"\n",
    "        self.is_training = False\n",
    "        if hasattr(self, 'training_thread') and self.training_thread.is_alive():\n",
    "            self.training_thread.join()\n",
    "\n",
    "class Visualizer:\n",
    "    def __init__(self, env, trainer):\n",
    "        self.env = env\n",
    "        self.trainer = trainer\n",
    "        self.fig = None\n",
    "        self.ani = None\n",
    "        self.is_running = False\n",
    "        \n",
    "        # Create figure with proper layout\n",
    "        self.setup_figure()\n",
    "    \n",
    "    def setup_figure(self):\n",
    "        \"\"\"Set up the figure layout with tank and metrics side by side\"\"\"\n",
    "        self.fig = plt.figure(figsize=(18, 8))\n",
    "        \n",
    "        # Create grid layout: 60% for tank, 40% for metrics\n",
    "        gs = GridSpec(1, 5, figure=self.fig)\n",
    "        \n",
    "        # Tank subplot (takes up 3/5 of the width)\n",
    "        self.ax_tank = self.fig.add_subplot(gs[0, :3])\n",
    "        self.ax_tank.set_xlim(0, self.env.tank_size)\n",
    "        self.ax_tank.set_ylim(0, self.env.tank_size)\n",
    "        self.ax_tank.set_aspect('equal')\n",
    "        self.ax_tank.set_title('Fish Tank Environment')\n",
    "        \n",
    "        # Metrics subplots (stacked vertically in remaining 2/5 of width)\n",
    "        gs_metrics = GridSpec(3, 1, height_ratios=[1, 1, 1])\n",
    "        self.ax_reward = self.fig.add_subplot(gs[0, 3:])\n",
    "        self.ax_distance = self.fig.add_subplot(gs_metrics[1, 0])\n",
    "        self.ax_loss = self.fig.add_subplot(gs_metrics[2, 0])\n",
    "        \n",
    "        # Set titles for metric plots\n",
    "        self.ax_reward.set_title('Episode Rewards')\n",
    "        self.ax_distance.set_title('Distance to Center')\n",
    "        self.ax_loss.set_title('Policy and Value Losses')\n",
    "        \n",
    "        # Labels\n",
    "        self.ax_reward.set_xlabel('Episode')\n",
    "        self.ax_reward.set_ylabel('Reward')\n",
    "        self.ax_distance.set_xlabel('Episode')\n",
    "        self.ax_distance.set_ylabel('Avg Distance')\n",
    "        self.ax_loss.set_xlabel('Episode')\n",
    "        self.ax_loss.set_ylabel('Loss')\n",
    "        \n",
    "        # Text for episode/step info\n",
    "        self.episode_text = self.ax_tank.text(\n",
    "            0.02, 0.98, '', transform=self.ax_tank.transAxes, \n",
    "            verticalalignment='top', fontsize=10\n",
    "        )\n",
    "        \n",
    "        # Adjust layout\n",
    "        self.fig.tight_layout()\n",
    "    \n",
    "    def init_animation(self):\n",
    "        \"\"\"Initialize animation elements\"\"\"\n",
    "        # Create tank background\n",
    "        tank_rect = patches.Rectangle(\n",
    "            (0, 0), self.env.tank_size, self.env.tank_size,\n",
    "            linewidth=2, edgecolor='black', facecolor=WATER_COLOR\n",
    "        )\n",
    "        self.ax_tank.add_patch(tank_rect)\n",
    "        \n",
    "        # Create center marker\n",
    "        center = self.env.center\n",
    "        center_marker = plt.Circle(center, 0.2, color='green', alpha=0.7)\n",
    "        self.ax_tank.add_patch(center_marker)\n",
    "        \n",
    "        # Create fish marker\n",
    "        self.fish = plt.Circle(self.env.position, 0.2, color=FISH_COLOR)\n",
    "        self.ax_tank.add_patch(self.fish)\n",
    "        \n",
    "        # Create current markers\n",
    "        self.current_markers = []\n",
    "        self.current_arrows = []\n",
    "        for i, current in enumerate(self.env.currents):\n",
    "            color = CURRENT_COLORS[i % len(CURRENT_COLORS)]\n",
    "            marker = plt.Circle(current.position, current.radius, color=color, alpha=0.5)\n",
    "            self.ax_tank.add_patch(marker)\n",
    "            self.current_markers.append(marker)\n",
    "            \n",
    "            # Add direction arrow\n",
    "            arrow = self.ax_tank.arrow(\n",
    "                current.position[0], current.position[1],\n",
    "                0.5*current.direction[0], 0.5*current.direction[1],\n",
    "                head_width=0.2, head_length=0.3, fc=color, ec=color\n",
    "            )\n",
    "            self.current_arrows.append(arrow)\n",
    "        \n",
    "        # Initialize trajectory line\n",
    "        self.trajectory_line, = self.ax_tank.plot([], [], 'b-', alpha=0.3)\n",
    "        \n",
    "        # Initialize reward plot\n",
    "        self.reward_line, = self.ax_reward.plot([], [], 'r-')\n",
    "        self.avg_reward_line, = self.ax_reward.plot([], [], 'g-')\n",
    "        self.ax_reward.legend(['Episode Reward', 'Avg Reward (10 ep)'])\n",
    "        \n",
    "        # Initialize distance plot\n",
    "        self.distance_line, = self.ax_distance.plot([], [], 'b-')\n",
    "        \n",
    "        # Initialize loss plot\n",
    "        self.policy_loss_line, = self.ax_loss.plot([], [], 'r-')\n",
    "        self.value_loss_line, = self.ax_loss.plot([], [], 'g-')\n",
    "        self.ax_loss.legend(['Policy Loss', 'Value Loss'])\n",
    "        \n",
    "        return [self.fish, self.trajectory_line, self.reward_line, \n",
    "                self.avg_reward_line, self.distance_line, \n",
    "                self.policy_loss_line, self.value_loss_line]\n",
    "    \n",
    "    def update_animation(self, frame):\n",
    "        \"\"\"Update the animation for each frame\"\"\"\n",
    "        # Only update visualization every VISUALIZATION_INTERVAL steps\n",
    "        if frame % VISUALIZATION_INTERVAL != 0:\n",
    "            return []\n",
    "        \n",
    "        with self.trainer.update_lock:\n",
    "            # Update fish position\n",
    "            self.fish.center = self.env.position\n",
    "            \n",
    "            # Update trajectory\n",
    "            if len(self.env.trajectory) > 0:\n",
    "                trajectory = np.array(self.env.trajectory)\n",
    "                self.trajectory_line.set_data(trajectory[:, 0], trajectory[:, 1])\n",
    "            \n",
    "            # Update currents\n",
    "            for i, (current, marker, arrow) in enumerate(zip(\n",
    "                    self.env.currents, self.current_markers, self.current_arrows)):\n",
    "                marker.center = current.position\n",
    "                marker.radius = current.radius\n",
    "                \n",
    "                # Remove old arrow and create new one\n",
    "                arrow.remove()\n",
    "                color = CURRENT_COLORS[i % len(CURRENT_COLORS)]\n",
    "                self.current_arrows[i] = self.ax_tank.arrow(\n",
    "                    current.position[0], current.position[1],\n",
    "                    0.5*current.direction[0], 0.5*current.direction[1],\n",
    "                    head_width=0.2, head_length=0.3, fc=color, ec=color\n",
    "                )\n",
    "            \n",
    "            # Update metrics plots if data available\n",
    "            if len(self.trainer.episode_rewards) > 0:\n",
    "                episodes = np.arange(len(self.trainer.episode_rewards))\n",
    "                self.reward_line.set_data(episodes, self.trainer.episode_rewards)\n",
    "                self.ax_reward.relim()\n",
    "                self.ax_reward.autoscale_view()\n",
    "                \n",
    "                if len(self.trainer.avg_rewards) > 0:\n",
    "                    self.avg_reward_line.set_data(episodes, self.trainer.avg_rewards)\n",
    "            \n",
    "            if len(self.trainer.distances_to_center) > 0:\n",
    "                episodes = np.arange(len(self.trainer.distances_to_center))\n",
    "                self.distance_line.set_data(episodes, self.trainer.distances_to_center)\n",
    "                self.ax_distance.relim()\n",
    "                self.ax_distance.autoscale_view()\n",
    "            \n",
    "            if len(self.trainer.policy_losses) > 0:\n",
    "                episodes = np.arange(len(self.trainer.policy_losses))\n",
    "                self.policy_loss_line.set_data(episodes, self.trainer.policy_losses)\n",
    "                self.value_loss_line.set_data(episodes, self.trainer.value_losses)\n",
    "                self.ax_loss.relim()\n",
    "                self.ax_loss.autoscale_view()\n",
    "            \n",
    "            # Update episode text\n",
    "            self.episode_text.set_text(\n",
    "                f'Episode: {self.trainer.current_episode + 1}\\n'\n",
    "                f'Step: {self.trainer.current_step}\\n'\n",
    "                f'Status: {\"Training\" if self.trainer.is_training else \"Testing\"}'\n",
    "            )\n",
    "        \n",
    "        return [self.fish, self.trajectory_line, self.reward_line, \n",
    "                self.avg_reward_line, self.distance_line, \n",
    "                self.policy_loss_line, self.value_loss_line]\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start the visualization\"\"\"\n",
    "        self.is_running = True\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.update_animation, init_func=self.init_animation,\n",
    "            frames=None, interval=50, blit=False\n",
    "        )\n",
    "        plt.show()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the visualization\"\"\"\n",
    "        self.is_running = False\n",
    "        if self.ani is not None:\n",
    "            self.ani.event_source.stop()\n",
    "\n",
    "def visualize_trained_agent(env, agent, num_episodes=5):\n",
    "    \"\"\"Visualize the performance of a trained agent\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.set_xlim(0, env.tank_size)\n",
    "    ax.set_ylim(0, env.tank_size)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('Trained Fish Agent Performance')\n",
    "    \n",
    "    # Create tank background\n",
    "    tank_rect = patches.Rectangle(\n",
    "        (0, 0), env.tank_size, env.tank_size,\n",
    "        linewidth=2, edgecolor='black', facecolor=WATER_COLOR\n",
    "    )\n",
    "    ax.add_patch(tank_rect)\n",
    "    \n",
    "    # Create center marker\n",
    "    center = env.center\n",
    "    center_marker = plt.Circle(center, 0.2, color='green', alpha=0.7)\n",
    "    ax.add_patch(center_marker)\n",
    "    \n",
    "    # Run episodes and plot trajectories\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        # Different color for each episode trajectory\n",
    "        color = plt.cm.jet(episode / num_episodes)\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from policy (deterministic)\n",
    "            action = agent.policy.get_action(state, deterministic=True)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, _, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Plot trajectory\n",
    "        if len(env.trajectory) > 0:\n",
    "            trajectory = np.array(env.trajectory)\n",
    "            ax.plot(trajectory[:, 0], trajectory[:, 1], '-', color=color, alpha=0.7, \n",
    "                   label=f'Episode {episode+1}')\n",
    "            \n",
    "            # Plot start and end points\n",
    "            ax.plot(trajectory[0, 0], trajectory[0, 1], 'o', color=color, markersize=8)\n",
    "            ax.plot(trajectory[-1, 0], trajectory[-1, 1], 's', color=color, markersize=8)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the fish tank environment and training\"\"\"\n",
    "    # Create environment\n",
    "    env = FishTankEnv(num_currents=3)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = FishTrainer(env)\n",
    "    \n",
    "    # Create visualizer\n",
    "    visualizer = Visualizer(env, trainer)\n",
    "    \n",
    "    # Start training in background thread\n",
    "    trainer.start_training_thread()\n",
    "    \n",
    "    try:\n",
    "        # Start visualization\n",
    "        visualizer.start()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user\")\n",
    "    finally:\n",
    "        # Stop training\n",
    "        trainer.stop_training()\n",
    "        visualizer.stop()\n",
    "    \n",
    "    # Load best model\n",
    "    if os.path.exists(trainer.model_path):\n",
    "        state_dict = torch.load(trainer.model_path)\n",
    "        trainer.agent.policy.load_state_dict(state_dict)\n",
    "        print(f\"Loaded best model from {trainer.model_path}\")\n",
    "        \n",
    "        # Visualize trained agent\n",
    "        visualize_trained_agent(env, trainer.agent)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bcc81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FishTank RL Combinedversion(revE)\n",
    "======================================\n",
    "A **complete, selfcontained** implementation for JupyterLab / VSCode\n",
    "notebooks *or* the regular Python interpreter. Run `run_visualisation()` in a\n",
    "cell (or execute the file) to start live training with an Advantage Actor\n",
    "Critic (A2C) agent steering a fish toward the tank centre while avoiding wall\n",
    "stickiness and drifting currents.\n",
    "\n",
    "Key points\n",
    "----------\n",
    "* Notebooksafe animation (`blit=False`, automatic `plt.ion()`).\n",
    "* Bigger fish marker **plus** a fading trail so motion is visible.\n",
    "* Metrics (mean reward & mean centredistance) plotted on the right.\n",
    "* Wallrepulsion buffer and exponential centre reward for smoother learning.\n",
    "* Clean shutdown: close the figure  stop training  save best net to\n",
    "  `best_fish_policy.pt`.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import threading, queue, math, warnings\n",
    "from collections import deque\n",
    "from typing import Deque, List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# -- Notebook friendliness ----------------------------------------------------\n",
    "if \"get_ipython\" in globals():\n",
    "    plt.ion()\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*cache_frame_data.*\")\n",
    "\n",
    "# -- Environment constants ----------------------------------------------------\n",
    "TANK_SIZE = 100.0\n",
    "CENTER = np.array([TANK_SIZE/2, TANK_SIZE/2])\n",
    "MAX_VELOCITY, MAX_FORCE = 30.0, 10.0\n",
    "DT = 0.1\n",
    "EPISODE_LEN = 1_000\n",
    "VISUALIZATION_INTERVAL = 10\n",
    "WALL_REPULSION = 50.0\n",
    "CURRENT_COUNT, MAX_CURRENT_STRENGTH, CURRENT_RADIUS = 1, 12.0, 25.0\n",
    "_rand_dir = lambda: np.random.normal(size=2) / np.linalg.norm(np.random.normal(size=2))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "class WaterCurrent:\n",
    "    def __init__(self):\n",
    "        self.position = np.random.uniform(0, TANK_SIZE, 2)\n",
    "        self.direction = _rand_dir()\n",
    "        self.strength  = np.random.uniform(3.0, MAX_CURRENT_STRENGTH)\n",
    "        self.radius = CURRENT_RADIUS\n",
    "    def vector(self, pos):\n",
    "        d = self.position - pos; dist = np.linalg.norm(d)\n",
    "        if dist > self.radius: return np.zeros(2)\n",
    "        return self.direction * self.strength * (1 - dist/self.radius)\n",
    "    def step(self):\n",
    "        self.position = (self.position + np.random.normal(0, 0.3, 2)) % TANK_SIZE\n",
    "        self.direction += 0.05 * _rand_dir(); self.direction /= np.linalg.norm(self.direction)\n",
    "\n",
    "class FishTankEnv:\n",
    "    obs_dim, act_dim = 4, 2\n",
    "    def __init__(self):\n",
    "        self.currents = [WaterCurrent() for _ in range(CURRENT_COUNT)]\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.position = np.random.uniform(0, TANK_SIZE, 2)\n",
    "        self.velocity = np.zeros(2); self.step_count = 0\n",
    "        return self._obs()\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -1, 1) * MAX_FORCE\n",
    "        force = action.copy() + sum(cur.vector(self.position) for cur in self.currents) + self._wall_force()\n",
    "        self.velocity += force * DT\n",
    "        s = np.linalg.norm(self.velocity)\n",
    "        if s > MAX_VELOCITY: self.velocity *= MAX_VELOCITY/s\n",
    "        self.position = (self.position + self.velocity*DT) % TANK_SIZE\n",
    "        if self.step_count % 5 == 0:\n",
    "            for c in self.currents: c.step()\n",
    "        self.step_count += 1\n",
    "        done = self.step_count >= EPISODE_LEN\n",
    "        return self._obs(), self._reward(), done, {}\n",
    "    # helpers\n",
    "    def _obs(self):\n",
    "        return np.concatenate([(self.position-CENTER)/(TANK_SIZE/2), self.velocity/MAX_VELOCITY]).astype(np.float32)\n",
    "    def _wall_force(self):\n",
    "        w = np.zeros(2)\n",
    "        if self.position[0] < 1: w[0] = WALL_REPULSION*(1-self.position[0])\n",
    "        elif self.position[0] > TANK_SIZE-1: w[0] = WALL_REPULSION*(TANK_SIZE-1-self.position[0])\n",
    "        if self.position[1] < 1: w[1] = WALL_REPULSION*(1-self.position[1])\n",
    "        elif self.position[1] > TANK_SIZE-1: w[1] = WALL_REPULSION*(TANK_SIZE-1-self.position[1])\n",
    "        return w\n",
    "    def _reward(self):\n",
    "        d = np.linalg.norm(self.position-CENTER)\n",
    "        return math.exp(-5*d/(TANK_SIZE/2)) - 0.1 - 0.001*np.linalg.norm(self.velocity)/MAX_VELOCITY\n",
    "\n",
    "# -- ActorCritic -------------------------------------------------------------\n",
    "\n",
    "def mlp(sizes, act=nn.Tanh, last=nn.Identity):\n",
    "    layers=[]\n",
    "    for i in range(len(sizes)-1): layers+= [nn.Linear(sizes[i], sizes[i+1]), (act if i<len(sizes)-2 else last)()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, o, a):\n",
    "        super().__init__(); h=128\n",
    "        self.pi = mlp([o,h,h,a], last=nn.Tanh); self.log_std = nn.Parameter(torch.zeros(a))\n",
    "        self.v  = mlp([o,h,h,1])\n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            mu = self.pi(obs); std = self.log_std.exp(); dist = torch.distributions.Normal(mu,std)\n",
    "            act = dist.sample(); logp = dist.log_prob(act).sum(-1); val = self.v(obs).squeeze(-1)\n",
    "        return act.numpy(), logp.numpy(), val.numpy()\n",
    "    def act(self, obs): return self.step(obs)[0]\n",
    "\n",
    "GAMMA, LAMBDA = 0.99, 0.97\n",
    "TRAIN_ITERS, BATCH_SIZE = 5, 4096\n",
    "POLICY_LR, VALUE_LR = 3e-4, 1e-3\n",
    "\n",
    "class A2CLearner:\n",
    "    def __init__(self):\n",
    "        self.env = FishTankEnv(); o,a = self.env.obs_dim, self.env.act_dim\n",
    "        self.ac = ActorCritic(o,a); self.pi_opt = optim.Adam(self.ac.parameters(), lr=POLICY_LR)\n",
    "        self.v_opt = optim.Adam(self.ac.v.parameters(), lr=VALUE_LR)\n",
    "        self.best_return=-np.inf; self.best_state=None\n",
    "        self.ep_returns, self.ep_dists = deque(maxlen=200), deque(maxlen=200)\n",
    "        self.metric_q: \"queue.Queue[tuple[float,float]]\" = queue.Queue()\n",
    "    def _gae(self,r,v,last,d):\n",
    "        adv=np.zeros_like(r); gae=0\n",
    "        for t in reversed(range(len(r))):\n",
    "            delta = r[t]+GAMMA*(v[t+1] if t+1<len(v) else last)*(1-d[t]) - v[t]\n",
    "            gae = delta + GAMMA*LAMBDA*(1-d[t])*gae; adv[t]=gae\n",
    "        return adv, adv+v[:-1]\n",
    "    def train_forever(self,stop):\n",
    "        while not stop.is_set():\n",
    "            ob,acv,lpv,rw,vv,dn=[],[],[],[],[],[]\n",
    "            obs=self.env.reset(); ep_r=ep_d=0\n",
    "            for _ in range(BATCH_SIZE):\n",
    "                o_t=torch.as_tensor(obs,dtype=torch.float32)\n",
    "                act,lp,val=self.ac.step(o_t)\n",
    "                nobs,rwd,done,_=self.env.step(act)\n",
    "                ob.append(obs); acv.append(act); lpv.append(lp); rw.append(rwd); vv.append(val); dn.append(done)\n",
    "                ep_r+=rwd; ep_d+=np.linalg.norm(self.env.position-CENTER); obs=nobs\n",
    "                if done:\n",
    "                    self.ep_returns.append(ep_r); self.ep_dists.append(ep_d/self.env.step_count)\n",
    "                    obs=self.env.reset(); ep_r=ep_d=0\n",
    "            last_val=self.ac.v(torch.as_tensor(obs,dtype=torch.float32)).item()\n",
    "            adv,ret=self._gae(np.array(rw,dtype=np.float32), np.array(vv,dtype=np.float32), last_val, np.array(dn,dtype=np.float32))\n",
    "            ob_t=torch.as_tensor(np.array(ob),dtype=torch.float32); ac_t=torch.as_tensor(np.array(acv),dtype=torch.float32)\n",
    "            lp_t=torch.as_tensor(np.array(lpv),dtype=torch.float32); adv_t=torch.as_tensor(adv,dtype=torch.float32)\n",
    "            ret_t=torch.as_tensor(ret,dtype=torch.float32)\n",
    "            for _ in range(TRAIN_ITERS):\n",
    "                mu=self.ac.pi(ob_t); std=self.ac.log_std.exp(); dist=torch.distributions.Normal(mu,std)\n",
    "                new_lp=dist.log_prob(ac_t).sum(-1); ratio=torch.exp(new_lp-lp_t); pi_loss=-(ratio*adv_t).mean()\n",
    "                self.pi_opt.zero_grad(); pi_loss.backward(); self.pi_opt.step()\n",
    "                v_pred=self.ac.v(ob_t).squeeze(-1); v_loss=((v_pred-ret_t)**2).mean()\n",
    "                self.v_opt.zero_grad(); v_loss.backward(); self.v_opt.step()\n",
    "            mret=np.mean(self.ep_returns) if self.ep_returns else -np.inf\n",
    "            if mret>self.best_return:\n",
    "                self.best_return=mret; self.best_state=self.ac.state_dict()\n",
    "            if not self.metric_q.full():\n",
    "                self.metric_q.put((mret, np.mean(self.ep_dists) if self.ep_dists else np.nan))\n",
    "    def load_best(self):\n",
    "        if self.best_state: self.ac.load_state_dict(self.best_state)\n",
    "\n",
    "# -- Visualisation ------------------------------------------------------------\n",
    "\n",
    "def run_visualisation():\n",
    "    learner=A2CLearner(); stop=threading.Event(); threading.Thread(target=learner.train_forever,args=(stop,),daemon=True).start()\n",
    "    fig=plt.figure(figsize=(10,6)); gs=GridSpec(1,2,width_ratios=[3,2])\n",
    "    ax_tank=fig.add_subplot(gs[0]); ax_tank.set_xlim(0,TANK_SIZE); ax_tank.set_ylim(0,TANK_SIZE); ax_tank.set_aspect('equal'); ax_tank.axis('off')\n",
    "    ax_metrics=fig.add_subplot(gs[1]); ax_metrics.set_title('Training metrics'); rew_hist,dist_hist=[],[]\n",
    "    fish_dot,=ax_tank.plot([],[],'bo',markersize=8)\n",
    "    trail,=ax_tank.plot([],[],'b-',alpha=0.3)\n",
    "    trail_buf=deque(maxlen=30)\n",
    "    arrows=[]\n",
    "    def update(_):\n",
    "        for _ in range(VISUALIZATION_INTERVAL):\n",
    "            o=learner.env._obs(); act=learner.ac.act(torch.as_tensor(o,dtype=torch.float32)); learner.env.step(act)\n",
    "        pos=learner.env.position; fish_dot.set_data(*pos); trail_buf.append(pos.copy());\n",
    "        if len(trail_buf)>1:\n",
    "            trail.set_data(*zip(*trail_buf))\n",
    "        for a in arrows: a.remove(); arrows.clear()\n",
    "        for cur in learner.env.currents:\n",
    "            vec=cur.direction*cur.strength; arr=patches.FancyArrow(cur.position[0],cur.position[1],vec[0],vec[1],width=1.2,color='cyan',alpha=0.6)\n",
    "            ax_tank.add_patch(arr); arrows.append(arr)\n",
    "        try:\n",
    "            while True:\n",
    "                mr,md=learner.metric_q.get_nowait(); rew_hist.append(mr); dist_hist.append(md)\n",
    "        except queue.Empty:\n",
    "            pass\n",
    "        ax_metrics.cla(); ax_metrics.set_title('Training metrics'); ax_metrics.plot(rew_hist,label='mean reward'); ax_metrics.plot(dist_hist,label='mean dist'); ax_metrics.legend()\n",
    "        return [fish_dot,trail]+arrows\n",
    "    ani=FuncAnimation(fig,update,interval=100,blit=False)\n",
    "    plt.show(); stop.set();\n",
    "    if learner.best_state:\n",
    "        learner.load_best(); torch.save(learner.best_state,'best_fish_policy.pt'); print(f'Best model saved (mean return {learner.best_return:.2f})')\n",
    "\n",
    "# Helper for script execution -------------------------------------------------\n",
    "if __name__=='__main__':\n",
    "    run_visualisation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e980c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
